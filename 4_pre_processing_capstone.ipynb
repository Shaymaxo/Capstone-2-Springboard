{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ6y8NrCGXT2",
        "outputId": "2bc234ff-ce74-440f-eba7-3e0651e04720"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Base directory where your project folders live\n",
        "drive_base = '/content/drive/MyDrive'\n",
        "# Path to the folder containing your EDA notebook\n",
        "eda_folder = os.path.join(drive_base, 'data', 'raw', 'Capstone 2 - Data Wrangling')\n",
        "eda_notebook_path = os.path.join(eda_folder, 'EDA new-capstone.ipynb')\n",
        "print('EDA notebook path:', eda_notebook_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MK-DgyngGaBr",
        "outputId": "14e26fa2-a663-45d7-abd3-7c42cceb803a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EDA notebook path: /content/drive/MyDrive/data/raw/Capstone 2 - Data Wrangling/EDA new-capstone.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 2: Load raw CSV files and merge\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "Mf3dJUq_Gagi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_path = '/content/drive/MyDrive/Capstone1/Capstone 2 - Data Wrangling/ieee-fraud-detection_project/data/raw/Archive.zip'\n",
        "extract_path = '/content/data_extracted'\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# 4. Extract the zip file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# 5. Optional: List the extracted files to see what’s inside\n",
        "os.listdir(extract_path)\n",
        "\n",
        "transaction_path = '/content/data_extracted/train_transaction.csv'\n",
        "identity_path = '/content/data_extracted/train_identity.csv'\n",
        "\n",
        "print(\"train_transaction.csv exists:\", os.path.exists(transaction_path))\n",
        "print(\"train_identity.csv exists:\", os.path.exists(identity_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvQDJL4jGai3",
        "outputId": "3f499da7-a724-4dcb-bd16-ff0312a67e5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_transaction.csv exists: True\n",
            "train_identity.csv exists: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "metadata": {
        "id": "koAKzxEGGalN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# 1) Set path to your archive\n",
        "zip_path = '/content/drive/MyDrive/Capstone1/Capstone 2 - Data Wrangling/ieee-fraud-detection_project/data/raw/Archive.zip'\n",
        "extract_path = '/content/fraud_data/'\n",
        "\n",
        "# 2) Extract files if not already extracted\n",
        "if not os.path.exists(extract_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "# 3) Load transaction and identity CSVs\n",
        "transaction_path = os.path.join(extract_path, 'train_transaction.csv')\n",
        "identity_path = os.path.join(extract_path, 'train_identity.csv')\n",
        "\n",
        "trans = pd.read_csv(transaction_path)\n",
        "iden = pd.read_csv(identity_path)\n",
        "\n",
        "# 4) Merge on TransactionID (left join to keep all transaction rows)\n",
        "df = trans.merge(iden, on=\"TransactionID\", how=\"left\")\n",
        "\n",
        "# 5) Split into fraud and non-fraud\n",
        "fraud_df = df[df[\"isFraud\"] == 1]\n",
        "nonfraud_df = df[df[\"isFraud\"] == 0]\n",
        "\n",
        "print(f\"Fraud count: {len(fraud_df):,}\")\n",
        "print(f\"Non-fraud count: {len(nonfraud_df):,}\")\n",
        "\n",
        "# 6) Sample non-fraud (you can adjust the ratio as needed)\n",
        "n_fraud = len(fraud_df)\n",
        "n_nonfraud_to_sample = n_fraud * 2  # change to 1x, 2x, 3x etc. based on RAM\n",
        "\n",
        "nonfraud_sampled = nonfraud_df.sample(n=n_nonfraud_to_sample, random_state=42)\n",
        "\n",
        "# 7) Combine both\n",
        "combined_df = pd.concat([fraud_df, nonfraud_sampled], axis=0).reset_index(drop=True)\n",
        "\n",
        "# 8) Shuffle\n",
        "combined_df = combined_df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(f\"Final combined dataset shape: {combined_df.shape}\")\n",
        "print(f\"Fraud ratio:\\n{combined_df['isFraud'].value_counts(normalize=True)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RP97bJUDOeV6",
        "outputId": "152482fc-30dd-4c65-a260-318a9b2a31d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fraud count: 20,663\n",
            "Non-fraud count: 569,877\n",
            "Final combined dataset shape: (61989, 434)\n",
            "Fraud ratio:\n",
            "isFraud\n",
            "0    0.666667\n",
            "1    0.333333\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original class imbalance:\n",
        "\n",
        "- Total transactions: 590,540\n",
        "- Fraudulent: 20,663 (≈ 3.5%)\n",
        "- Non-fraudulent: 569,877 (≈ 96.5%)\n",
        "\n",
        "Undersampling strategy:\n",
        "\n",
        "Kept all 20,663 fraud cases.\n",
        "Randomly sampled 41,326 non-fraud cases (2 × fraud count).\n",
        "\n",
        "Final combined dataset:\n",
        "\n",
        "- 61,989 transactions, 434 features\n",
        "- Fraud = 33.3 %\n",
        "- Non-fraud = 66.7 %\n",
        "\n",
        "This rebalanced dataset both reduces computational overhead and provides a more representative class distribution, helping downstream classification algorithms learn to detect fraud without being overwhelmed by the majority class."
      ],
      "metadata": {
        "id": "dSPWwSNM6722"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# 1) Assume `combined_df` is already in memory (from merging, sampling, etc.)\n",
        "#    If not, load it here. For example:\n",
        "# combined_df = pd.read_csv(\"/path/to/your/combined_df.csv\")\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# 2) Separate target\n",
        "y = combined_df[\"isFraud\"]\n",
        "X = combined_df.drop(columns=[\"isFraud\", \"TransactionID\"])  # drop ID + target"
      ],
      "metadata": {
        "id": "Z9EmuxNpRNzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Drop columns with > 90% missing values\n",
        "missing_pct = X.isna().mean()\n",
        "high_missing_cols = missing_pct[missing_pct > 0.90].index.tolist()\n",
        "print(f\"Dropping {len(high_missing_cols)} columns with > 90% missing:\\n{high_missing_cols}\\n\")\n",
        "X = X.drop(columns=high_missing_cols)\n",
        "\n",
        "# 4) Identify numeric vs. categorical columns\n",
        "#    (We treat anything with dtype \"object\" as categorical. Adjust if needed.)\n",
        "numeric_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "cat_cols     = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "\n",
        "print(f\"Numeric features (n={len(numeric_cols)}): {numeric_cols[:10]} ...\")\n",
        "print(f\"Categorical features (n={len(cat_cols)}): {cat_cols[:10]} ...\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJCU2NGtRRWP",
        "outputId": "517f7242-b5fa-4193-a898-7904c4fa06cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropping 9 columns with > 90% missing:\n",
            "['id_07', 'id_08', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27']\n",
            "\n",
            "Numeric features (n=394): ['TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'dist1', 'dist2'] ...\n",
            "Categorical features (n=29): ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5'] ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Build transformers for each type\n",
        "\n",
        "# 5a) Numeric imputer + (optional) scaler\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    # If you want to scale: uncomment the next line\n",
        "    # (\"scaler\", StandardScaler()),\n",
        "])\n",
        "\n",
        "# 5b) Categorical imputer + ordinal encoder\n",
        "#     - imputing missing with a constant string \"__MISSING__\"\n",
        "#     - ordinal encoder will assign integers to each category\n",
        "cat_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"__MISSING__\")),\n",
        "    (\"encoder\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
        "])\n"
      ],
      "metadata": {
        "id": "bobJOYADRRdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 6) Bundle into a ColumnTransformer\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    (\"num\", numeric_transformer, numeric_cols),\n",
        "    (\"cat\", cat_transformer,     cat_cols),\n",
        "], remainder=\"drop\")  # we already dropped anything we don't want\n",
        "\n",
        "# 7) Split into train/test (stratify on 'isFraud')\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.20,\n",
        "    stratify=y,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Before preprocessing → Train shape: {X_train.shape},  Test shape: {X_test.shape}\")\n",
        "print(f\"Train class distribution:\\n{y_train.value_counts(normalize=True)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3ekAMe8RRgQ",
        "outputId": "00c4075c-6cda-4847-ba9d-3606baae99f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before preprocessing → Train shape: (49591, 423),  Test shape: (12398, 423)\n",
            "Train class distribution:\n",
            "isFraud\n",
            "0    0.666673\n",
            "1    0.333327\n",
            "Name: proportion, dtype: float64\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dropped 9 columns with >90% missing values.\n",
        "- Numeric features (394) imputed with median; categorical features (29) imputed with constant and ordinal encoded.\n",
        "- Data split stratified on isFraud with 80% train (49,591 samples) and 20% test (12,398 samples).\n",
        "- Training set class distribution: 66.7% non-fraud, 33.3% fraud.\n",
        "\n",
        "Preprocessing ensures clean, encoded data with preserved class balance, ready for modeling."
      ],
      "metadata": {
        "id": "u1eycXPj7J85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8) Fit & transform training data; transform test data\n",
        "X_train_prep = preprocessor.fit_transform(X_train)\n",
        "X_test_prep  = preprocessor.transform(X_test)\n",
        "\n",
        "print(f\"After preprocessing → X_train_prep shape: {X_train_prep.shape}\")\n",
        "print(f\"                       X_test_prep  shape: {X_test_prep.shape}\\n\")\n",
        "\n",
        "# 9) Apply SMOTE to the training set ONLY\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train_prep, y_train)\n",
        "\n",
        "print(\"After SMOTE →\")\n",
        "print(f\"  X_train_smote shape: {X_train_smote.shape}\")\n",
        "print(f\"  y_train_smote distribution:\\n{pd.Series(y_train_smote).value_counts(normalize=True)}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klX4vDdjRRi2",
        "outputId": "0e408838-8f29-4b18-9477-f43b5ad27c8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After preprocessing → X_train_prep shape: (49591, 423)\n",
            "                       X_test_prep  shape: (12398, 423)\n",
            "\n",
            "After SMOTE →\n",
            "  X_train_smote shape: (66122, 423)\n",
            "  y_train_smote distribution:\n",
            "isFraud\n",
            "0    0.5\n",
            "1    0.5\n",
            "Name: proportion, dtype: float64\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Preprocessed training data shape: (49,591, 423)\n",
        "\n",
        "- Preprocessed test data shape: (12,398, 423)\n",
        "\n",
        "After applying SMOTE on training set:\n",
        "\n",
        "- Samples increased to 66,122\n",
        "- Perfectly balanced classes: 50% fraud, 50% non-fraud\n",
        "\n",
        "This approach addresses class imbalance effectively, providing a balanced and comprehensive dataset for model training."
      ],
      "metadata": {
        "id": "P4lnhKJR7TIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 11) Save the processed datasets to disk\n",
        "# Feel free to change to your preferred format (CSV, pickle, joblib, etc.)\n",
        "pd.DataFrame(X_train_smote).to_csv(\"X_train_smote.csv\", index=False)\n",
        "pd.Series(y_train_smote, name=\"isFraud\").to_csv(\"y_train_smote.csv\", index=False)\n",
        "\n",
        "pd.DataFrame(X_test_prep).to_csv(\"X_test_processed.csv\", index=False)\n",
        "pd.Series(y_test, name=\"isFraud\").to_csv(\"y_test.csv\", index=False)\n",
        "\n",
        "print(\"Preprocessed files written:\")\n",
        "print(\"  - X_train_smote.csv, y_train_smote.csv\")\n",
        "print(\"  - X_test_processed.csv, y_test.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSDjUlJpRRnr",
        "outputId": "9ef697d3-31ef-437a-b750-85b0299f7a01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed files written:\n",
            "  - X_train_smote.csv, y_train_smote.csv\n",
            "  - X_test_processed.csv, y_test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('/content/drive/MyDrive/Capstone1/processed', exist_ok=True)\n"
      ],
      "metadata": {
        "id": "SCJbyTp7Sth6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save preprocessed (before SMOTE)\n",
        "joblib.dump(X_train_prep, '/content/drive/MyDrive/Capstone1/processed/X_train_prep.pkl')\n",
        "joblib.dump(X_test_prep, '/content/drive/MyDrive/Capstone1/processed/X_test_prep.pkl')\n",
        "joblib.dump(y_train, '/content/drive/MyDrive/Capstone1/processed/y_train.pkl')\n",
        "joblib.dump(y_test, '/content/drive/MyDrive/Capstone1/processed/y_test.pkl')\n",
        "\n",
        "# Save SMOTE-balanced training data\n",
        "joblib.dump(X_train_smote, '/content/drive/MyDrive/Capstone1/processed/X_train_smote.pkl')\n",
        "joblib.dump(y_train_smote, '/content/drive/MyDrive/Capstone1/processed/y_train_smote.pkl')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1Y3INRGS1RQ",
        "outputId": "38867990-152d-46d1-9bd8-310cab328171"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/Capstone1/processed/y_train_smote.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jWd7tvb9S3xJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}