{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvYW6sn1U/5NYm3ylYLrEe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shaymaxo/Capstone-2-Springboard/blob/main/3_pre_processing_capstone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ6y8NrCGXT2",
        "outputId": "117dda1a-b8e8-4e0c-a748-c5cb240d696d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Base directory where your project folders live\n",
        "drive_base = '/content/drive/MyDrive'\n",
        "# Path to the folder containing your EDA notebook\n",
        "eda_folder = os.path.join(drive_base, 'data', 'raw', 'Capstone 2 - Data Wrangling')\n",
        "eda_notebook_path = os.path.join(eda_folder, 'EDA new-capstone.ipynb')\n",
        "print('EDA notebook path:', eda_notebook_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MK-DgyngGaBr",
        "outputId": "82fbde97-b0ff-44b6-da50-60aff5071edf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EDA notebook path: /content/drive/MyDrive/data/raw/Capstone 2 - Data Wrangling/EDA new-capstone.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 2: Load raw CSV files and merge\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "Mf3dJUq_Gagi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_path = '/content/drive/MyDrive/Capstone1/Capstone 2 - Data Wrangling/ieee-fraud-detection_project/data/raw/Archive.zip'\n",
        "extract_path = '/content/data_extracted'\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# 4. Extract the zip file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# 5. Optional: List the extracted files to see what’s inside\n",
        "os.listdir(extract_path)\n",
        "\n",
        "transaction_path = '/content/data_extracted/train_transaction.csv'\n",
        "identity_path = '/content/data_extracted/train_identity.csv'\n",
        "\n",
        "print(\"train_transaction.csv exists:\", os.path.exists(transaction_path))\n",
        "print(\"train_identity.csv exists:\", os.path.exists(identity_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvQDJL4jGai3",
        "outputId": "ad6fc6dd-e48f-4a36-b610-ba8767c99611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_transaction.csv exists: True\n",
            "train_identity.csv exists: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "metadata": {
        "id": "koAKzxEGGalN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To keep memory usage manageable and avoid crashing the Colab environment, I limited the data load to the first 100,000 rows of each CSV file by using the nrows parameter with `pd.read_csv()`.\n",
        "\n",
        "I then merged the two datasets — train_transaction.csv and train_identity.csv — on their common column TransactionID using a left join. This approach preserves all transaction records while adding identity information wherever available. The resulting dataframe, `df`, has 100,000 rows and 434 columns, incorporating both transaction and identity features. This merged dataset is essential for providing a comprehensive view of each transaction, combining all the relevant data necessary for the fraud detection model."
      ],
      "metadata": {
        "id": "URfaMfqaLQ8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load and merge data\n",
        "n_rows = 100000\n",
        "transaction_df = pd.read_csv(f'{extract_path}/train_transaction.csv', nrows=n_rows)\n",
        "identity_df = pd.read_csv(f'{extract_path}/train_identity.csv', nrows=n_rows)\n",
        "df = transaction_df.merge(identity_df, how='left', on='TransactionID')\n",
        "\n",
        "print(\"Merged shape:\", df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikobGqDlGan0",
        "outputId": "0a6c44ff-75d2-4d75-905b-4ff4f1ae7969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged shape: (100000, 434)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Separate target variable\n",
        "y = df['isFraud']\n",
        "X = df.drop(['isFraud'], axis=1)\n",
        "\n",
        "# 4. Encode categorical features\n",
        "categorical_cols = ['ProductCD', 'card4', 'card6', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6',\n",
        "                    'M7', 'M8', 'M9', 'id_12', 'id_15', 'id_16', 'id_28', 'id_29',\n",
        "                    'id_35', 'DeviceType']\n",
        "\n",
        "X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)"
      ],
      "metadata": {
        "id": "QFu6cZngGtrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 80% of the data goes into X_train and y_train (for training the model)\n",
        "* 20% goes into X_test and y_test (for evaluating how well the model generalizes).\n",
        "\n",
        "I included two important parameters:\n",
        "\n",
        "- `stratify=y`: This ensures that the class distribution of the target variable `isFraud` is preserved in both the train and test sets. Since fraud is a highly imbalanced class (only ~2.5% are frauds), stratification is critical.\n",
        "\n",
        "- `random_state=42`: This ensures that the split is reproducible every time I run the code."
      ],
      "metadata": {
        "id": "ad9W5CiRNalH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Handle missing values (simple version: fill with -999 or median)\n",
        "X = X.fillna(-999)\n",
        "\n",
        "# 6. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "Ki6GuqI6Gtts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the types of columns\n",
        "print(X_train.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJ1kaE8jGtyH",
        "outputId": "3d95db01-33e5-4fcf-e8d8-28bd8c76fe0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TransactionID          int64\n",
            "TransactionDT          int64\n",
            "TransactionAmt       float64\n",
            "card1                  int64\n",
            "card2                float64\n",
            "                      ...   \n",
            "id_16_NotFound          bool\n",
            "id_28_New               bool\n",
            "id_29_NotFound          bool\n",
            "id_35_T                 bool\n",
            "DeviceType_mobile       bool\n",
            "Length: 441, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If there's still text data, we need to handle it by encoding it (we'll use get_dummies for all categorical columns)\n",
        "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
        "X_train = pd.get_dummies(X_train, columns=categorical_cols, drop_first=True)\n",
        "X_test = pd.get_dummies(X_test, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# Re-align the columns in X_train and X_test in case there are mismatches after one-hot encoding\n",
        "X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)\n",
        "\n",
        "# Now we can apply scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "B1eykC-9Gt0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Class Distribution in Train and Test Sets\n",
        "I checked the class distribution of the target variable (isFraud) in both the training and test sets. The output shows that both the training and test sets have a significant class imbalance, with the majority class (non-fraudulent transactions) comprising approximately 97.4%, and the minority class (fraudulent transactions) only around 2.6%.\n",
        "\n",
        "***Train set class distribution:***\n",
        "\n",
        "* Non-fraudulent (0): 97.44%\n",
        "* Fraudulent (1): 2.56%\n",
        "\n",
        "***Test set class distribution:***\n",
        "\n",
        "* Non-fraudulent (0): 97.44%\n",
        "* Fraudulent (1): 2.56%\n",
        "\n",
        "Given this imbalance, the model might have a tendency to predict the majority class (non-fraudulent) more frequently, which could lead to poor detection of fraudulent transactions. This highlights the need for techniques like SMOTE to address the class imbalance before training the model."
      ],
      "metadata": {
        "id": "htUY7sV4PDOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check class distribution in train and test sets\n",
        "print(\"Train set class distribution:\")\n",
        "print(y_train.value_counts(normalize=True))\n",
        "\n",
        "print(\"\\nTest set class distribution:\")\n",
        "print(y_test.value_counts(normalize=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfDllHaBPAcY",
        "outputId": "a578b6e6-e4d2-4e19-f342-f05a4c788ca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set class distribution:\n",
            "isFraud\n",
            "0    0.974387\n",
            "1    0.025612\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Test set class distribution:\n",
            "isFraud\n",
            "0    0.9744\n",
            "1    0.0256\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧼 Balancing the Dataset and Finalizing Preprocessed DataFrames\n",
        "\n",
        "To address the severe class imbalance in my training dataset—where fraudulent transactions made up only around 2.56% of the data—I applied SMOTE (Synthetic Minority Over-sampling Technique) to the training data only. The final print statement confirmed that the training set was now balanced, with the output showing an equal distribution. This preprocessing ensures that the data is clean, scaled, balanced, and ready for model training.\n",
        "\n"
      ],
      "metadata": {
        "id": "qdGFf_r3OzDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Apply SMOTE to training data only\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "fpGOvPodGap5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Wrap everything into DataFrames again (optional, for convenience)\n",
        "X_train_df = pd.DataFrame(X_train_resampled, columns=X_train.columns)\n",
        "X_test_df = pd.DataFrame(X_test_scaled, columns=X_train.columns)\n",
        "y_train_df = pd.DataFrame(y_train_resampled, columns=['isFraud'])\n",
        "y_test_df = pd.DataFrame(y_test.values, columns=['isFraud'])\n",
        "\n",
        "# 10. Check final class balance\n",
        "print(\"Final class distribution in training set after SMOTE:\")\n",
        "print(y_train_df['isFraud'].value_counts(normalize=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OviDwe80JIUA",
        "outputId": "6d4abf4d-c197-4c0e-98d9-6a2c41f059c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final class distribution in training set after SMOTE:\n",
            "isFraud\n",
            "0    0.5\n",
            "1    0.5\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8nrcya1tJIWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mKCF6AP7JIZR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}