{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMiXfHFcaC1bW/CUbyGZInF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shaymaxo/Capstone-2-Springboard/blob/main/4_modeling_capstone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal: Develop a final model that predicts fraudulent transactions using two to three different algorithms, apply hyperparameter tuning, and define metrics for model selection.\n",
        "\n",
        "## Modeling Setup and Metrics Definition\n",
        "\n",
        "\n",
        "**Modeling Goal:** Build and compare multiple classification models.\n",
        "\n",
        "**Algorithms:** Logistic Regression, Random Forest, XGBoost\n",
        "\n",
        "**Hyperparameter Tuning:** GridSearchCV with cross-validation\n",
        "\n",
        "**Evaluation Metrics:** Accuracy, Precision, Recall, F1-score, ROC AUC\n",
        "\n",
        "**Final Selection Criteria:** Balance between predictive performance, computational efficiency, and scalability.\n"
      ],
      "metadata": {
        "id": "OGtQh83P7x9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1w_xlj5tVuyj",
        "outputId": "869962e1-6a9e-41bb-ebb5-92f47d43fbb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Base directory where your project folders live\n",
        "drive_base = '/content/drive/MyDrive'\n",
        "# Path to the folder containing your EDA notebook\n",
        "pre_processing_folder = os.path.join(drive_base, 'data', 'raw', 'Capstone 2 - Data Wrangling')\n",
        "pre_processing_notebook_path = os.path.join(pre_processing_folder, '3. pre-processing capstone.ipynb')\n",
        "print('Pre-processing notebook path:', pre_processing_notebook_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxqYHRULViVu",
        "outputId": "84fecaa9-bbe9-45ed-c7c6-e7509a9035fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-processing notebook path: /content/drive/MyDrive/data/raw/Capstone 2 - Data Wrangling/3. pre-processing capstone.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# 2. Imports and Configuration\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_auc_score, classification_report, precision_recall_curve)\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.sparse import issparse"
      ],
      "metadata": {
        "id": "y7-RxnPx-V7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "## 3. Load and Sample Data\n",
        "\n",
        "# %%\n",
        "data_path = \"/content/drive/MyDrive/Capstone1/Capstone 2 - Data Wrangling/ieee-fraud-detection_project/data/raw/processed_fraud_data.csv\"\n",
        "if not os.path.exists(data_path):\n",
        "    raise FileNotFoundError(f\"Data not found at {data_path}\")\n",
        "\n",
        "df = pd.read_csv(data_path)\n",
        "if 'Unnamed: 0' in df.columns:\n",
        "    df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "_, df_sampled = train_test_split(df, train_size=40000, stratify=df['isFraud'], random_state=42)\n",
        "X_full = df_sampled.drop(columns=['isFraud'])\n",
        "y_full = df_sampled['isFraud']\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_full, y_full, test_size=0.2, stratify=y_full, random_state=42\n",
        ")\n",
        "print(\"Training set:\", X_train.shape, \"Test set:\", X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWaDJCh-PLdv",
        "outputId": "c861055a-23b8-4492-baa9-e253a877e2d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: (48000, 433) Test set: (12000, 433)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset was successfully loaded and stratified to maintain the original class distribution of fraudulent vs. non-fraudulent transactions. A sample of 40,000 transactions was used to optimize performance, then split into training and test sets in an 80/20 ratio. This resulted in 48,000 training samples and 12,000 test samples, each with 433 features, ensuring a balanced and efficient foundation for modeling."
      ],
      "metadata": {
        "id": "hdMUGCra1dho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "## 4. Preprocessing and Dimensionality Reduction\n",
        "\n",
        "# Identify column types\n",
        "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "num_cols = X_train.select_dtypes(include=['number']).columns.tolist()\n",
        "\n",
        "numeric_transformer = Pipeline([('imputer', SimpleImputer(strategy='mean')),\n",
        "                                 ('scaler', StandardScaler())])\n",
        "cat_transformer = Pipeline([('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "                            ('encoder', OneHotEncoder(handle_unknown='ignore'))])\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', numeric_transformer, num_cols),\n",
        "    ('cat', cat_transformer, cat_cols)\n",
        "])\n",
        "\n",
        "def transform_dense(prep, X, fit=True):\n",
        "    Xt = prep.fit_transform(X) if fit else prep.transform(X)\n",
        "    if issparse(Xt):\n",
        "        Xt = Xt.toarray()\n",
        "    return Xt.astype(np.float32)\n",
        "\n",
        "X_train_prep = transform_dense(preprocessor, X_train, fit=True)\n",
        "X_test_prep = transform_dense(preprocessor, X_test, fit=False)\n",
        "\n",
        "# Apply PCA to speed up baseline\n",
        "pca = PCA(n_components=50, random_state=42)\n",
        "X_train_reduced = pca.fit_transform(X_train_prep)\n",
        "X_test_reduced = pca.transform(X_test_prep)\n",
        "print(f\"Reduced features: {X_train_reduced.shape[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ql2RY_aPLgD",
        "outputId": "9a600d63-b155-4629-9058-702be6d23d23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reduced features: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data was successfully preprocessed using imputation, scaling, and one-hot encoding to handle missing values and categorical features. To improve computational efficiency, PCA was applied, reducing the feature space from 433 to 50 components while preserving key variance, making the dataset more manageable for baseline model training."
      ],
      "metadata": {
        "id": "KXm-ZwOh1l7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "## 5. Baseline Models (No Tuning) on Reduced Data\n",
        "\n",
        "# Switch to liblinear for faster Logistic Regression\n",
        "models = {\n",
        "    'LogisticRegression': LogisticRegression(solver='liblinear', max_iter=200, class_weight='balanced', random_state=42),\n",
        "    'RandomForest': RandomForestClassifier(n_estimators=50, max_depth=10, class_weight='balanced', random_state=42),\n",
        "    'XGBoost': XGBClassifier(n_estimators=50, max_depth=6, learning_rate=0.1, subsample=0.7,\n",
        "                              colsample_bytree=0.7, scale_pos_weight=1, use_label_encoder=False,\n",
        "                              eval_metric='logloss', random_state=42)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_reduced, y_train)\n",
        "    y_pred = model.predict(X_test_reduced)\n",
        "    y_proba = model.predict_proba(X_test_reduced)[:,1]\n",
        "    results[name] = {\n",
        "        'Accuracy': accuracy_score(y_test, y_pred),\n",
        "        'Precision': precision_score(y_test, y_pred),\n",
        "        'Recall': recall_score(y_test, y_pred),\n",
        "        'F1': f1_score(y_test, y_pred),\n",
        "        'ROC AUC': roc_auc_score(y_test, y_proba)\n",
        "    }\n",
        "\n",
        "baseline_df = pd.DataFrame(results).T\n",
        "print(baseline_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61mx-7pFRowc",
        "outputId": "f46bc9c3-ca79-40c7-fe0a-213e02212025"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [21:36:19] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    Accuracy  Precision    Recall        F1   ROC AUC\n",
            "LogisticRegression  0.777417   0.078760  0.719870  0.141985  0.817217\n",
            "RandomForest        0.945333   0.228616  0.478827  0.309474  0.852522\n",
            "XGBoost             0.978417   0.875000  0.182410  0.301887  0.847730\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Three baseline models—Logistic Regression, Random Forest, and XGBoost—were evaluated on the reduced dataset. Logistic Regression achieved **high recall (0.72)** but suffered from **low precision and F1 score**, indicating many false positives. Random Forest and XGBoost **outperformed in overall accuracy (94.5% and 97.8%)** and AUC, with XGBoost showing **very high precision (0.88)** but **low recall**, suggesting it's more conservative in flagging fraud. This highlights the classic trade-off between detecting fraud and minimizing false alarms."
      ],
      "metadata": {
        "id": "8whX0kWq1uw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "## 6. Hyperparameter Tuning (RandomizedSearchCV)\n",
        "\n",
        "# Tune only RandomForest with a slightly broader grid\n",
        "from scipy.stats import randint\n",
        "\n",
        "rf_param_dist = {\n",
        "    'n_estimators': randint(50, 100),\n",
        "    'max_depth': [8, 10, 12],\n",
        "    'class_weight': ['balanced', 'balanced_subsample']\n",
        "}\n",
        "\n",
        "ts = RandomizedSearchCV(\n",
        "    models['RandomForest'],\n",
        "    param_distributions=rf_param_dist,\n",
        "    n_iter=10,\n",
        "    scoring='roc_auc',\n",
        "    cv=2,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "ts.fit(X_train_reduced, y_train)\n",
        "\n",
        "# Evaluate tuned RandomForest\n",
        "tuned_rf = ts.best_estimator_\n",
        "y_pred_rf = tuned_rf.predict(X_test_reduced)\n",
        "y_proba_rf = tuned_rf.predict_proba(X_test_reduced)[:,1]\n",
        "\n",
        "random_results = {\n",
        "    'RandomForest': {\n",
        "        'BestParams': ts.best_params_,\n",
        "        'Accuracy': accuracy_score(y_test, y_pred_rf),\n",
        "        'Precision': precision_score(y_test, y_pred_rf),\n",
        "        'Recall': recall_score(y_test, y_pred_rf),\n",
        "        'F1': f1_score(y_test, y_pred_rf),\n",
        "        'ROC AUC': roc_auc_score(y_test, y_proba_rf)\n",
        "    }\n",
        "}\n",
        "random_df = pd.DataFrame(random_results).T\n",
        "print(random_df)\n",
        "\n",
        "# Adjust threshold to improve recall\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_test, y_proba_rf)\n",
        "target_recall = 0.6\n",
        "best_idx = np.argmax(recalls >= target_recall)\n",
        "if best_idx < len(thresholds):\n",
        "    custom_thresh = thresholds[best_idx]\n",
        "    print(f\"Selected threshold: {custom_thresh:.2f} for recall: {recalls[best_idx]:.3f}\")\n",
        "\n",
        "    y_pred_thresh = (y_proba_rf >= custom_thresh).astype(int)\n",
        "    print(\"Adjusted threshold metrics:\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred_thresh))\n",
        "    print(\"Precision:\", precision_score(y_test, y_pred_thresh))\n",
        "    print(\"Recall:\", recall_score(y_test, y_pred_thresh))\n",
        "    print(\"F1:\", f1_score(y_test, y_pred_thresh))\n",
        "    print(\"ROC AUC:\", roc_auc_score(y_test, y_proba_rf))\n",
        "else:\n",
        "    print(\"No threshold found for desired recall.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mSmsz03-V-i",
        "outputId": "b7ee2397-2e88-4062-c6d7-02e3288928d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                     BestParams  Accuracy  \\\n",
            "RandomForest  {'class_weight': 'balanced', 'max_depth': 8, '...  0.901583   \n",
            "\n",
            "             Precision    Recall       F1  ROC AUC  \n",
            "RandomForest  0.144715  0.579805  0.23162  0.84229  \n",
            "Selected threshold: 0.06 for recall: 1.000\n",
            "Adjusted threshold metrics:\n",
            "Accuracy: 0.025583333333333333\n",
            "Precision: 0.025583333333333333\n",
            "Recall: 1.0\n",
            "F1: 0.04989030632973105\n",
            "ROC AUC: 0.8422898969872841\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tuning of the Random Forest modestly improved its detection ability—**raising recall to 0.58 while holding ROC AUC at 0.84**—at the expense of some precision (0.14). Pushing the decision threshold to achieve perfect recall (1.0) confirmed the extreme trade-off: precision and accuracy both fell to ~2.6%, illustrating that maximizing fraud capture can overwhelm false positives. This underscores the need to balance threshold selection to meet real-world business requirements."
      ],
      "metadata": {
        "id": "D8RpVoyo18sX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Threshold fine-tuning on tuned RandomForest\n",
        "probas = y_proba_rf\n",
        "thresholds = np.linspace(0, 1, 101)\n",
        "best_f1 = 0.0\n",
        "best_thresh = 0.5\n",
        "\n",
        "for t in thresholds:\n",
        "    y_pred_t = (probas >= t).astype(int)\n",
        "    f1 = f1_score(y_test, y_pred_t)\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_thresh = t\n",
        "\n",
        "print(f\"Best threshold by F1: {best_thresh:.2f} with F1: {best_f1:.4f}\")\n",
        "\n",
        "# Evaluate at best threshold\n",
        "y_pred_thresh = (probas >= best_thresh).astype(int)\n",
        "print(\"Metrics at best threshold:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_thresh))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred_thresh))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_thresh))\n",
        "print(\"F1:\", f1_score(y_test, y_pred_thresh))\n",
        "print(\"ROC AUC (proba):\", roc_auc_score(y_test, probas))\n",
        "# -------------------------------\n",
        "\n",
        "# %% [markdown]\n",
        "## 7. Ensemble Model: VotingClassifier\n",
        "\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Create a soft-voting ensemble of the tuned RandomForest and your XGBoost model\n",
        "ensemble = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('rf', tuned_rf),\n",
        "        ('xgb', models['XGBoost'])  # use your baseline XGBoost or a tuned version\n",
        "    ],\n",
        "    voting='soft',  # uses predicted probabilities\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "ensemble.fit(X_train_reduced, y_train)\n",
        "y_pred_ens = ensemble.predict(X_test_reduced)\n",
        "y_proba_ens = ensemble.predict_proba(X_test_reduced)[:, 1]\n",
        "\n",
        "ensemble_results = {\n",
        "    'Ensemble': {\n",
        "        'Accuracy': accuracy_score(y_test, y_pred_ens),\n",
        "        'Precision': precision_score(y_test, y_pred_ens),\n",
        "        'Recall': recall_score(y_test, y_pred_ens),\n",
        "        'F1': f1_score(y_test, y_pred_ens),\n",
        "        'ROC AUC': roc_auc_score(y_test, y_proba_ens)\n",
        "    }\n",
        "}\n",
        "\n",
        "ensemble_df = pd.DataFrame(ensemble_results).T\n",
        "print(ensemble_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXXOjbyQR-ZU",
        "outputId": "0f2316f4-a424-4c7d-e0a4-28bbcc36a2c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best threshold by F1: 0.69 with F1: 0.3872\n",
            "Metrics at best threshold:\n",
            "Accuracy: 0.9744166666666667\n",
            "Precision: 0.5\n",
            "Recall: 0.31596091205211724\n",
            "F1: 0.3872255489021956\n",
            "ROC AUC (proba): 0.8422898969872841\n",
            "          Accuracy  Precision   Recall        F1   ROC AUC\n",
            "Ensemble   0.97975   0.796296  0.28013  0.414458  0.846561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizing the Random Forest decision threshold by F1 score identified 0.69 as the sweet spot—boosting F1 to 0.387 with 50% precision and 32% recall while maintaining ROC AUC at 0.842. Building a soft-voting ensemble of this tuned RF and the XGBoost model further improved overall separation (ROC AUC 0.847) and F1 (0.414), with precision climbing to 0.80 though recall dipped to 0.28. In practice, the threshold-tuned RF balances fraud detection and false alarms effectively, while the ensemble offers higher confidence in flagged cases at the expense of missing some frauds.\n"
      ],
      "metadata": {
        "id": "wEGZ84rA2Ih6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "## 7.1 Ensemble Model (Voting)\n",
        "\n",
        "#To balance precision and recall, we build a soft-voting ensemble of the tuned RandomForest and a tuned XGBoost (using the same reduced data and fast-mode parameters).\n",
        "\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Re-initialize XGBoost with fast-mode tuned parameters\n",
        "xgb_fast = XGBClassifier(\n",
        "    n_estimators=30, max_depth=6, learning_rate=0.1,\n",
        "    subsample=0.7, colsample_bytree=0.7, scale_pos_weight=1,\n",
        "    use_label_encoder=False, eval_metric='logloss', random_state=42\n",
        ")\n",
        "\n",
        "# Fit XGBoost on reduced data\n",
        "xgb_fast.fit(X_train_reduced, y_train)\n",
        "\n",
        "# Assemble voting ensemble\n",
        "ensemble = VotingClassifier(\n",
        "    estimators=[('rf', tuned_rf), ('xgb', xgb_fast)],\n",
        "    voting='soft', weights=[1,1], n_jobs=-1\n",
        ")\n",
        "\n",
        "ensemble.fit(X_train_reduced, y_train)\n",
        "\n",
        "# Evaluate ensemble\n",
        "y_pred_ens = ensemble.predict(X_test_reduced)\n",
        "y_proba_ens = ensemble.predict_proba(X_test_reduced)[:,1]\n",
        "\n",
        "print(\"Ensemble Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_ens))\n",
        "print(\"Ensemble ROC AUC:\", roc_auc_score(y_test, y_proba_ens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgaNUEEPR-ee",
        "outputId": "3e6613df-56ff-4be8-ca40-9b4b5097e469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [22:28:49] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99     11693\n",
            "           1       0.82      0.24      0.38       307\n",
            "\n",
            "    accuracy                           0.98     12000\n",
            "   macro avg       0.90      0.62      0.68     12000\n",
            "weighted avg       0.98      0.98      0.97     12000\n",
            "\n",
            "Ensemble ROC AUC: 0.8452512444456453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The soft-voting ensemble of the tuned Random Forest and a fast-mode XGBoost model achieved a strong overall ROC AUC of 0.845, with precision at 82%—meaning most flagged transactions are true frauds—while recall remained modest at 24%, indicating it still misses a majority of fraud cases. Its F1-score of 0.38 reflects this trade-off, balancing high confidence in detected fraud against lower coverage. This ensemble is well suited for scenarios where prioritizing precision (i.e., minimizing false alarms) is critical, though further threshold tuning or additional techniques would be needed to improve recall if catching a higher fraction of frauds is the primary goal."
      ],
      "metadata": {
        "id": "ayckz9X12S40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "## 8. SMOTE + RandomForest Pipeline\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "# Build an imbalanced-aware pipeline\n",
        "smote_rf_pipe = ImbPipeline([\n",
        "    ('smote', SMOTE(sampling_strategy=0.5, random_state=42)),\n",
        "    # 0.5 means after SMOTE, fraud samples = 50% of non‐fraud\n",
        "    ('rf', RandomForestClassifier(\n",
        "        n_estimators=80, max_depth=10,\n",
        "        class_weight='balanced_subsample',\n",
        "        random_state=42, n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Fit on the original reduced training data\n",
        "smote_rf_pipe.fit(X_train_reduced, y_train)\n",
        "\n",
        "# Evaluate on the test set\n",
        "y_pred_smote = smote_rf_pipe.predict(X_test_reduced)\n",
        "y_proba_smote = smote_rf_pipe.predict_proba(X_test_reduced)[:,1]\n",
        "\n",
        "print(\"SMOTE + RF Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_smote))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba_smote))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVePOxVidgD3",
        "outputId": "107b98b6-e608-49ab-9649-b6e4b48aa6e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SMOTE + RF Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.89      0.94     11693\n",
            "           1       0.13      0.62      0.21       307\n",
            "\n",
            "    accuracy                           0.88     12000\n",
            "   macro avg       0.56      0.75      0.57     12000\n",
            "weighted avg       0.97      0.88      0.92     12000\n",
            "\n",
            "ROC AUC: 0.8494792535749696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By applying SMOTE to oversample the minority class to 50% of the majority and then fitting a Random Forest, recall was substantially increased to 62%, ensuring the model captures more fraud instances. However, precision dropped to 13%, reflecting many false positives, and overall accuracy fell to 88%. The ROC AUC improved slightly to 0.849, indicating better class separation. This approach is valuable when maximizing fraud detection (recall) is paramount, but may require further tuning or downstream filtering to manage the higher false‐positive rate."
      ],
      "metadata": {
        "id": "9fvlCL_a2eJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "## 9. Stacking Ensemble\n",
        "\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "# Base learners\n",
        "estimators = [\n",
        "    ('lr', models['LogisticRegression']),       # your liblinear LR\n",
        "    ('rf', tuned_rf),                            # your tuned RF\n",
        "    ('xgb', models['XGBoost'])                  # your fast‐mode XGBoost\n",
        "]\n",
        "\n",
        "stack = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression(solver='liblinear', max_iter=200),\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    passthrough=False  # if True, passes original features too\n",
        ")\n",
        "\n",
        "# Fit stacking ensemble\n",
        "stack.fit(X_train_reduced, y_train)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred_stack = stack.predict(X_test_reduced)\n",
        "y_proba_stack = stack.predict_proba(X_test_reduced)[:,1]\n",
        "\n",
        "print(\"Stacking Ensemble Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_stack))\n",
        "print(\"Stacking ROC AUC:\", roc_auc_score(y_test, y_proba_stack))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMtgywCgdgGr",
        "outputId": "fed278c6-399d-4d87-c6f0-9650ed3f189f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Ensemble Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99     11693\n",
            "           1       0.84      0.23      0.37       307\n",
            "\n",
            "    accuracy                           0.98     12000\n",
            "   macro avg       0.91      0.62      0.68     12000\n",
            "weighted avg       0.98      0.98      0.97     12000\n",
            "\n",
            "Stacking ROC AUC: 0.8458597824751634\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The stacking ensemble of Logistic Regression, tuned Random Forest, and XGBoost achieved a ROC AUC of 0.846, indicating strong overall discrimination between fraud and non-fraud. It delivered high precision (84%), meaning most flagged transactions are true positives, but modest recall (23%), capturing less than a quarter of actual fraud cases. Its F1 score (0.37) reflects this precision–recall trade-off. This ensemble is ideal when false positives are particularly costly and you need high confidence in each flagged transaction, though additional threshold tuning or complementary methods would be needed to improve its coverage of fraud cases."
      ],
      "metadata": {
        "id": "j_zHskdx2mDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary for the base and tuned metrics\n",
        "model_metrics = {\n",
        "    'LogisticRegression': {\n",
        "        'Base': {'Accuracy': 0.777417, 'Precision': 0.078760, 'Recall': 0.719870, 'F1': 0.141985, 'ROC AUC': 0.817217},\n",
        "        'Tuned': {'Accuracy': None, 'Precision': None, 'Recall': None, 'F1': None, 'ROC AUC': None, 'BestParams': None}\n",
        "    },\n",
        "    'RandomForest': {\n",
        "        'Base': {'Accuracy': 0.945333, 'Precision': 0.228616, 'Recall': 0.478827, 'F1': 0.309474, 'ROC AUC': 0.852522},\n",
        "        'Tuned': {'Accuracy': 0.901583, 'Precision': 0.144715, 'Recall': 0.579805, 'F1': 0.23162, 'ROC AUC': 0.84229, 'BestParams': {'class_weight': 'balanced', 'max_depth': 8, 'n_estimators': 100}}\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'Base': {'Accuracy': 0.978417, 'Precision': 0.875000, 'Recall': 0.182410, 'F1': 0.301887, 'ROC AUC': 0.847730},\n",
        "        'Tuned': {'Accuracy': None, 'Precision': None, 'Recall': None, 'F1': None, 'ROC AUC': None, 'BestParams': None}\n",
        "    },\n",
        "    'SMOTE+RF': {\n",
        "        'Base': {'Accuracy': None, 'Precision': None, 'Recall': None, 'F1': None, 'ROC AUC': None},\n",
        "        'Tuned': {'Accuracy': 0.88, 'Precision': 0.13, 'Recall': 0.62, 'F1': 0.21, 'ROC AUC': 0.8495}\n",
        "    },\n",
        "    'Stacking': {\n",
        "        'Base': {'Accuracy': None, 'Precision': None, 'Recall': None, 'F1': None, 'ROC AUC': None},\n",
        "        'Tuned': {'Accuracy': 0.98, 'Precision': 0.84, 'Recall': 0.23, 'F1': 0.37, 'ROC AUC': 0.8459}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Convert to DataFrame for comparison\n",
        "df_compare = pd.DataFrame.from_dict({model: {metric: values['Base'][metric] for metric in values['Base']}\n",
        "                                     for model, values in model_metrics.items()},\n",
        "                                    orient='index')\n",
        "\n",
        "df_tuned = pd.DataFrame.from_dict({model: {metric: values['Tuned'][metric] for metric in values['Tuned']}\n",
        "                                  for model, values in model_metrics.items()},\n",
        "                                  orient='index')\n",
        "\n",
        "# Merge base and tuned DataFrames\n",
        "df_compare_final = df_compare.join(df_tuned, lsuffix='_Base', rsuffix='_Tuned')\n",
        "\n",
        "# Display the final DataFrame\n",
        "print(df_compare_final)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKp7_aTDdgJS",
        "outputId": "7673963f-c2fe-43a9-ba12-2e27db8eb894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    Accuracy_Base  Precision_Base  Recall_Base   F1_Base  \\\n",
            "LogisticRegression       0.777417        0.078760     0.719870  0.141985   \n",
            "RandomForest             0.945333        0.228616     0.478827  0.309474   \n",
            "XGBoost                  0.978417        0.875000     0.182410  0.301887   \n",
            "SMOTE+RF                      NaN             NaN          NaN       NaN   \n",
            "Stacking                      NaN             NaN          NaN       NaN   \n",
            "\n",
            "                    ROC AUC_Base  Accuracy_Tuned  Precision_Tuned  \\\n",
            "LogisticRegression      0.817217             NaN              NaN   \n",
            "RandomForest            0.852522        0.901583         0.144715   \n",
            "XGBoost                 0.847730             NaN              NaN   \n",
            "SMOTE+RF                     NaN        0.880000         0.130000   \n",
            "Stacking                     NaN        0.980000         0.840000   \n",
            "\n",
            "                    Recall_Tuned  F1_Tuned  ROC AUC_Tuned  \\\n",
            "LogisticRegression           NaN       NaN            NaN   \n",
            "RandomForest            0.579805   0.23162        0.84229   \n",
            "XGBoost                      NaN       NaN            NaN   \n",
            "SMOTE+RF                0.620000   0.21000        0.84950   \n",
            "Stacking                0.230000   0.37000        0.84590   \n",
            "\n",
            "                                                           BestParams  \n",
            "LogisticRegression                                               None  \n",
            "RandomForest        {'class_weight': 'balanced', 'max_depth': 8, '...  \n",
            "XGBoost                                                          None  \n",
            "SMOTE+RF                                                          NaN  \n",
            "Stacking                                                          NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The consolidated results table shows that among all approaches, the threshold‐tuned Random Forest delivers the best balance of metrics for fraud detection: it achieves a solid ROC AUC of 0.842, an F1‐score of 0.232, and improved recall (0.580) over the untuned version, while maintaining reasonable precision. The SMOTE+RF pipeline maximizes recall (0.62) but at the expense of precision (0.13) and overall accuracy, making it useful only when catching nearly all fraud cases is critical. The stacking ensemble delivers the highest precision (0.84) and slightly higher ROC AUC (0.846) but lower recall (0.23), which is suitable when false positives must be minimized. Given the need to both detect a meaningful proportion of frauds and limit false alarms, the threshold‐tuned Random Forest emerges as the most effective final model."
      ],
      "metadata": {
        "id": "a1H_xlD62rke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final Model Comparison**\n",
        "\n",
        "After systematically evaluating a variety of models - including base and tuned versions of Logistic Regression, Random Forest, and XGBoost, as well as ensemble methods (Voting, Stacking) and imbalance-handling strategies (SMOTE)—the results reveal key trade-offs between precision, recall, and overall robustness.\n",
        "\n",
        "*   **Tuned Random Forest** stands out as the **most balanced model**. It achieves a **recall of 0.58**, meaning it captures more than half of the fraudulent transactions, while still preserving reasonable **precision (0.14)** and **ROC AUC (0.84)**. This makes it effective for real-world deployment, where missing too many fraud cases can be costly.\n",
        "*   **SMOTE+RF** improves recall slightly (0.62) but significantly sacrifices precision (0.13), making it better suited for high-recall use cases like internal alerts - where false positives are acceptable as long as fraud is caught.\n",
        "\n",
        "*   **Stacking Ensemble** has the highest precision (0.84), but a recall of just 0.23. This model may be better suited when **false positives are costly**, such as in customer-facing scenarios.\n",
        "*   **Threshold tuning** was a critical step in improving fraud capture rates, highlighting the importance of going beyond default probability thresholds for classification in imbalanced datasets.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4JLf-tTf24eR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Top 3 Recommended Next Steps\n",
        "\n",
        "\n",
        "1.   **Deploy the Threshold-Tuned Random Forest Model**: It offers the best balance of fraud detection (recall) and minimizing false positives (precision), making it the most practical choice for production.\n",
        "2.   **Build and Package a Full Inference Pipeline**: Integrate all pre-processing (e.g., scaling, encoding, feature selection) with the final model to ensure consistent, end-to-end predictions in deployment.\n",
        "\n",
        "1.   **Validate with End Users**: I recommend collaborating with fraud analysts or domain experts to validate prediction quality, interpretability, and thresholds in real-world use cases.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0zbCJpzC6Esc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With these steps, the modeling phase will effectively transition into robust, interpretable, and maintainable fraud-detection operations."
      ],
      "metadata": {
        "id": "gJQPpKvH8rVY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nH1c5pST7sxP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}